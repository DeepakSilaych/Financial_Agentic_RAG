from typing import List, Optional

from pydantic import BaseModel, Field
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import AIMessage, RemoveMessage
from utils import log_message
from config import NUM_PREV_MESSAGES
import state
from state import QuestionNode, OverallState
from llm import llm
from prompt import prompts
from retriever import cache_retriever
import uuid , nodes 
from utils import send_logs
from config import LOGGING_SETTINGS
import config

# HELPER FUNCTIONS


def get_questions_and_answers_by_layer(
    root: QuestionNode, target_layer: int
) -> List[QuestionNode]:
    result = {}

    # If the current node is at the target layer, add its question and answer
    if root.layer == target_layer:
        result[root.question] = root.answer

    # Recursively search in children
    for child in root.children:
        result.update(get_questions_and_answers_by_layer(child, target_layer))

    return result


def format_child_questions_and_answers(child_questions, child_answers):
    """
    Formats child questions and answers into a single string.

    :param child_questions: List of child questions.
    :param child_answers: List of corresponding child answers.
    :return: A formatted string combining the questions and answers.
    """
    return "\n".join(
        f"{i+1}. {question}: {answer}"
        for i, (question, answer) in enumerate(zip(child_questions, child_answers))
    )


def reset_state_except_final_and_messages(state: OverallState):
    """
    Resets all keys in the `OverallState` dictionary except for `final_answer` and `messages`.

    Args:
        state (OverallState): The state dictionary to be modified.
    """
    for key in state.keys():
        if key not in {"final_answer", "messages"}:
            # Reset lists and strings appropriately
            if isinstance(state[key], list):
                state[key] = []
            elif isinstance(state[key], str):
                state[key] = ""


def delete_messages(state):
    messages = state["messages"]
    if len(messages) > NUM_PREV_MESSAGES:
        return {
            "messages": [RemoveMessage(id=m.id) for m in messages[:-NUM_PREV_MESSAGES]]
        }


# STRUCTURED OUTPUTS


class DecomposedQuestions(BaseModel):
    """The structure for decomposed question generated by the decomposing the original question."""

    decomposed_questions: List[str] = Field(
        description="Contains the subquestions that have been generated by decomposing the original question."
    )


class DecomposedQuestionGroups(BaseModel):
    """The structure for decomposed question generated by the decomposing the original question."""

    decomposed_question_groups: List[List[str]] = Field(
        description="Contains the groups of subquestions that have been generated by decomposing the original question."
    )


class SufficientAnswer(BaseModel):
    """Yes or No if the original question based on qa pairs"""

    justification: str = Field(description="Reasoning for why the output was so")
    sufficient_answer: str = Field(
        description="Yes or No if the original question based on qa pairs"
    )


class Critic_Suggestion(BaseModel):
    """The structure for suggestion given by the critic after seeing the questions split from the original query."""

    critic_suggestion: str = Field(
        description="The combined question generated from the given question and answer of the previous question."
    )


class CombinedQuestion(BaseModel):
    """The structure for combined question generated by the combining the question and answer of the previous question."""

    combined_question: str = Field(
        description="The combined question generated from the given question and answer of the previous question."
    )


class CombinedAnswer(BaseModel):
    """The structure for combined answer generated by the combining the answers of the decomposed questions."""

    combined_answer: str = Field(
        description="The combined answer generated from the given set of answers of the decomposed questions."
    )


class CacheSufficient(BaseModel):
    """The Construct to determine if there is an answer which fully answers the question or not"""

    index: int = Field(description="Gives back the index of the right answer")


### TEMPLATE:
# @system prompt
# @final prompt
# @invoker
# @callable function (optional)

### 1 - question_decomposer [PARALLEL WORKFLOW]
## Splits the initial question to a series of subquestions

_decomposer_system_prompt = prompts._decomposer_system_prompt
question_decomposition_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", _decomposer_system_prompt),
        ("human", "Question: {question}"),
    ]
)

question_decomposer = question_decomposition_prompt | llm.with_structured_output(
    DecomposedQuestions
)


def decompose_question(state: state.OverallState):
    """
    Decompose the user question into a series of subquestions to optimize document retrieval.
    """
    log_message("---DECOMPOSE QUESTION---")

    question = state["question"]

    decomposed_questions = question_decomposer.invoke({"question": question})
    decomposed_questions = decomposed_questions.decomposed_questions

    log_message(f"Decomposed questions: {decomposed_questions}")

    return {"decomposed_questions": decomposed_questions}


### 2 - question_decomposer_v2 [SERIES - PARALLEL]
## Splits into groups of sequential sbu-questions

_decomposer_system_prompt_v2 = prompts._decomposer_system_prompt_v2

question_decomposition_prompt_v2 = ChatPromptTemplate.from_messages(
    [
        ("system", _decomposer_system_prompt_v2),
        ("human", "Question: {question}"),
    ]
)

question_decomposer_v2 = question_decomposition_prompt_v2 | llm.with_structured_output(
    DecomposedQuestionGroups
)


def decompose_question_v2(state: state.OverallState):
    """
    Decompose the user question into a series of subquestions to optimize document retrieval.
    """
    log_message("---DECOMPOSING QUESTION IN SERIES AND PARALLEL---")

    question = state["question"]

    decomposed_questions_groups = question_decomposer_v2.invoke({"question": question})
    decomposed_questions_groups = decomposed_questions_groups.decomposed_question_groups

    log_message(f"------Decomposed question groups: {decomposed_questions_groups}")

    return {"decomposed_question_groups": decomposed_questions_groups}


### 3 - question_decomposer_v3 [CONTREGEN]
## At every node, this decomposer splits the questions into a list of simpler questions

_decomposer_system_prompt_v3 = prompts._decomposer_system_prompt_v3

question_decomposition_prompt_v3 = ChatPromptTemplate.from_messages(
    [
        ("system", _decomposer_system_prompt_v3),
        ("human", "Question: {question}"),
    ]
)

question_decomposer_v3 = question_decomposition_prompt_v3 | llm.with_structured_output(
    DecomposedQuestions
)


### 4 - question_decomposer_v4 [CRITIC]
## Decomposes the question taking in input (suggestion) from the critic agent

_decomposer_system_prompt_v4 = prompts._decomposer_system_prompt_v4

question_decomposition_prompt_v4 = ChatPromptTemplate.from_messages(
    [
        ("system", _decomposer_system_prompt_v4),
        (
            "human",
            " Original Question: {question} \n Previous Subquestions: {prev_subquestions} Suggestion: {suggestion} \n Output: \n",
        ),
    ]
)

question_decomposer_v4 = question_decomposition_prompt_v4 | llm.with_structured_output(
    DecomposedQuestionGroups
)


def decompose_question_v4(state: state.OverallState):
    """
    Decompose the user question into a series of subquestions to optimize document retrieval, but taking in suggestions from a critic
    """
    log_message("---DECOMPOSING QUESTION IN SERIES AND PARALLEL---")

    question = state["question"]
    suggestion = state.get("critic_suggestion", "")
    prev_subquestions = state.get("decomposed_question_groups", [[]])

    decomposed_questions_groups = question_decomposer_v4.invoke(
        {
            "question": question,
            "suggestion": suggestion,
            "prev_subquestions": prev_subquestions,
        }
    )
    decomposed_questions_groups = decomposed_questions_groups.decomposed_question_groups

    log_message(f"------Decomposed question groups: {decomposed_questions_groups}")

    return {"decomposed_question_groups": decomposed_questions_groups}


### 5 - question_decomposer_v5 [REPEATER LAYER 1]
## Breaks a question into easily solvable parts, optimised for RAG

_decomposer_system_prompt_v5 = prompts._decomposer_system_prompt_v5

question_decomposition_prompt_v5 = ChatPromptTemplate.from_messages(
    [
        ("system", _decomposer_system_prompt_v5),
        ("human", "##INPUT: {question}\n##OUTPUT:"),
    ]
)

question_decomposer_v5 = question_decomposition_prompt_v5 | llm.with_structured_output(
    DecomposedQuestions
)


### 6 - question_decomposer_v6 [REPEATER LAYER 2 AND 3]
## Breaks a question into more easily solvable parts, but making sure not to produce similar questions from the history

_decomposer_system_prompt_v6 = prompts._decomposer_system_prompt_v6

question_decomposition_prompt_v6 = ChatPromptTemplate.from_messages(
    [
        ("system", _decomposer_system_prompt_v6),
        (
            "human",
            "##INPUT\n#Question: {question}\n#Subquestions: \n{sub_ques}  ##OUTPUT",
        ),
    ]
)

question_decomposer_v6 = question_decomposition_prompt_v6 | llm.with_structured_output(
    DecomposedQuestions
)


### check_sufficient [REPEATER]
## Given the question and list of q-a pairs, it returns if there is enough information to fully solve the question

sufficient_answer_prompt_system = prompts.sufficient_answer_prompt_system

sufficient_answer_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", sufficient_answer_prompt_system),
        ("human", "##Input:\nQuestion: {question}\n {qa_pairs}\n##Output: "),
    ]
)

check_sufficient = sufficient_answer_prompt | llm.with_structured_output(
    SufficientAnswer
)


### generator_critic - [GENERATOR_CRITIC]
## Provides the suggestion (the critic function)

_generator_critic_system = prompts._generator_critic_system

generator_critic_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", _generator_critic_system),
        (
            "human",
            " Original Question: {question} , Proposed Subquestions: {question_groups}",
        ),
    ]
)

generator_critic = generator_critic_prompt | llm.with_structured_output(
    Critic_Suggestion
)


def critic_node(state: state.OverallState):
    """
    Provides a suggestion from the critic based on the split of the
    """
    log_message("---Generator Critic---")

    question = state["question"]
    question_groups = state["decomposed_question_groups"]

    state["critic_counter"] = state.get("critic_counter", 0)
    state["critic_counter"] += 1

    question_groups = "\n".join([str(sublist) for sublist in question_groups])

    suggestion = generator_critic.invoke(
        {"question": question, "question_groups": question_groups}
    )
    suggestion = suggestion.critic_suggestion

    state["critic_suggestion"] = suggestion

    log_message(
        f"------Critic Suggestion: {suggestion}  \n Critic Count: {state['critic_counter']}"
    )

    return state


## QUESTION COMBINED V1


### question_combiner [GENERATOR CRITIC AND SERIES-PARALLEL]

_question_combiner_system_prompt = prompts._question_combiner_system_prompt

question_combination_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", _question_combiner_system_prompt),
        (
            "human",
            """Here is the previous question and its answer:
{prev_question}: {prev_answer}

Here is the follow up question question: {next_question}""",
        ),
    ]
)

question_combiner = question_combination_prompt | llm.with_structured_output(
    CombinedQuestion
)


### question_combiner_v1 [CONTREGEN, CONTREGENV2]

_question_combiner_system_prompt_v1 = prompts._question_combiner_system_prompt_v1

question_combination_prompt_v1 = ChatPromptTemplate.from_messages(
    [
        ("system", _question_combiner_system_prompt_v1),
        (
            "human",
            """Here are the child questions and their respective answers:
{child_questions_and_answers}

Here is the next question to address:
{next_question}""",
        ),
    ]
)

question_combiner_v1 = question_combination_prompt_v1 | llm.with_structured_output(
    CombinedQuestion
)


def combine_questions(child_questions, child_answers, next_question):
    """
    Combines child questions, answers, and a follow-up question into a new concise question.

    :param child_questions: List of child questions.
    :param child_answers: List of answers corresponding to the child questions.
    :param next_question: The follow-up question for the next step.
    :return: The combined question.
    """
    # Format child questions and answers
    child_qa_formatted = format_child_questions_and_answers(
        child_questions, child_answers
    )

    # Generate the new question
    combined_question_input = {
        "child_questions_and_answers": child_qa_formatted,
        "next_question": next_question,
    }

    return question_combiner_v1.invoke(combined_question_input).combined_question


### question_combiner_v2 [NOT BEING USED ANYWHERE]

_question_combiner_system_prompt_v2 = prompts._question_combiner_system_prompt

question_combination_prompt_v2 = ChatPromptTemplate.from_messages(
    [
        ("system", _question_combiner_system_prompt_v2),
        (
            "human",
            """Here are the child questions and their answers:
{child_questions_answers}

Here is the follow-up question:{original_question}""",
        ),
    ]
)

question_combiner_v2 = question_combination_prompt_v2 | llm.with_structured_output(
    CombinedQuestion
)


### question_combiner_v3 [REPEATER]

_question_combiner_system_prompt_v3 = prompts._question_combiner_system_prompt_v3

question_combination_prompt_v3 = ChatPromptTemplate.from_messages(
    [
        ("system", _question_combiner_system_prompt_v3),
        (
            "human",
            """##INPUT
Original Question:{question}
#Subquestions and Answers:
{child_questions_and_answers}

##OUTPUT:""",
        ),
    ]
)

question_combiner_v3 = question_combination_prompt_v3 | llm.with_structured_output(
    CombinedQuestion
)


def combine_questions_v3(qa_pairs, question):
    """
    Combines child questions, answers, and a follow-up question into a new concise question.

    :param child_questions: List of child questions.
    :param child_answers: List of answers corresponding to the child questions.
    :param next_question: The follow-up question for the next step.
    :return: The combined question.
    """
    # Format child questions and answers

    # Generate the new question
    combined_question_input = {
        "question": question,
        "child_questions_and_answers": qa_pairs,
    }

    return question_combiner_v3.invoke(combined_question_input).combined_question


## answer_combiner [CONTREGEN_V2]

_answer_combiner_system_prompt = prompts._answer_combiner_system_prompt_qd

answer_combination_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", _answer_combiner_system_prompt),
        (
            "human",
            """Here are the child questions and their respective answers:
{child_questions_and_answers}

Here is the next question to address:
{next_question}""",
        ),
    ]
)

answer_combiner = answer_combination_prompt | llm.with_structured_output(CombinedAnswer)


def combine_answer(child_questions, child_answers, next_question):
    """
    Combines child answers into a single answer that addresses the follow-up question.

    :param child_questions: List of child questions.
    :param child_answers: List of answers corresponding to the child questions.
    :param next_question: The follow-up question for the next step.
    :return: A combined answer.
    """
    # Format child questions and answers
    child_qa_formatted = format_child_questions_and_answers(
        child_questions, child_answers
    )

    # Prepare the input for the prompt
    combined_answer_input = {
        "child_questions_and_answers": child_qa_formatted,
        "next_question": next_question,
    }

    # Invoke the prompt to generate the combined answer

    return answer_combiner.invoke(combined_answer_input).combined_answer


### answers_combiner and answers_combiner_with_image
# combine_answer_v1 -> [E2E, CRITIC, PERSONA WITH SUPERVISOR, SERIES PARALLEL]
# combine_answer_v2 -> [CONTREGEN_V1, CONTREGEN_V2]

_answer_combiner_system_prompt1 = prompts._answer_combiner_system_prompt1

answers_combination_prompt1 = ChatPromptTemplate.from_messages(
    [
        ("system", _answer_combiner_system_prompt1),
        (
            "human",
            """Here are the decomposed questions and their answers: {decomposed_qa_pairs}

Here is the original question: {original_question}""",
        ),
    ]
)

answers_combiner = answers_combination_prompt1 | llm.with_structured_output(
    CombinedAnswer
)


def combine_answer_v1(state: state.OverallState):
    """
    Combines the answers of the decomposed questions into a single final answer.
    """
    log_message("---COMBINING ALL THE DECOMPOSED ANSWERS---")

    original_question = state["question"]
    decomposed_questions = state["decomposed_questions"]
    decomposed_answers = state["decomposed_answers"]

    decomposed_qa_pairs = []
    for question, answer in zip(decomposed_questions, decomposed_answers):
        decomposed_qa_pairs.append(f"{question}: {answer}")
    decomposed_qa_pairs = "\n".join(decomposed_qa_pairs)

    image_url=state.get("image_url","")
    if state.get("image_url","")!="":
        image_url = f"data:image/jpeg;base64,{image_url}"
        image_desc=state.get("image_desc","")
        answers_combination_prompt_with_image = ChatPromptTemplate.from_messages(
            messages = [
                SystemMessage(content=_answer_combiner_system_prompt),
                HumanMessage(content=[
                    {
                        "type": "text",
                        "text": f"Question Answer Pairs:\n{decomposed_qa_pairs}\n",
                    },
                    {
                        "type": "image_url",
                        "image_url": {f"url: {image_url}"}
                    },
                    {
                        "type": "text",
                        "text": f"Image Description : {image_desc}"
                    },
                    {
                        "type": "text",
                        "text": f"Question: {original_question}"
                    }
                ]
                )
            ]
        )
        answers_combiner_with_image = answers_combination_prompt_with_image | llm.with_structured_output(
            CombinedAnswer
        )
        combined_answer=answers_combiner_with_image.invoke({})
    else:
        combined_answer = answers_combiner.invoke(
            {
                "original_question": original_question,
                "decomposed_qa_pairs": decomposed_qa_pairs,
            }
        )
    combined_answer = combined_answer.combined_answer

    log_message(f"Combined answer: {combined_answer}")
    delete_messages(state)
    reset_state_except_final_and_messages(state)

    return {
        "final_answer": combined_answer,
        "messages": [AIMessage(role="Chatbot", content=combined_answer)],
        "clarifying_questions": [],
    }


def combine_answer_v2(state: state.OverallState):
    log_message("---COMBINING ALL THE DECOMPOSED ANSWERS---")

    original_question = state["question"]
    question_tree = state["question_tree"]
    decomposed_qa_pairs = get_questions_and_answers_by_layer(question_tree, 1)
    image_url=state.get("image_url","")
    if state.get("image_url","")!="":
        image_url = f"data:image/jpeg;base64,{image_url}"
        image_desc=state.get("image_desc","")
        answers_combination_prompt_with_image = ChatPromptTemplate.from_messages(
            messages = [
                SystemMessage(content=_answer_combiner_system_prompt),
                HumanMessage(content=[
                    {
                        "type": "text",
                        "text": f"Question Answer Pairs:\n{decomposed_qa_pairs}\n",
                    },
                    {
                        "type": "image_url",
                        "image_url": {f"url: {image_url}"}
                    },
                    {
                        "type": "text",
                        "text": f"Question: {original_question}"
                    },
                    {
                        "type": "text",
                        "text": f"Image Description : {image_desc}"
                    },
                ]
                )
            ]
        )
        answers_combiner_with_image = answers_combination_prompt_with_image | llm.with_structured_output(
            CombinedAnswer
        )
        combined_answer=answers_combiner_with_image.invoke({})
    else:
        combined_answer = answers_combiner.invoke(
            {
                "original_question": original_question,
                "decomposed_qa_pairs": decomposed_qa_pairs,
            }
        )
    combined_answer = combined_answer.combined_answer

    log_message(f"Combined answer: {combined_answer}")
    delete_messages(state)
    reset_state_except_final_and_messages(state)

    return {
        "final_answer": combined_answer,
        "messages": [AIMessage(role="Chatbot", content=combined_answer)],
        "clarifying_questions": [],
    }


### answers_combiner2 and answers_combiner2_with_image [REPEATER]

_answer_combiner_system_prompt2 = prompts._answer_combiner_system_prompt2

answers_combination_prompt2 = ChatPromptTemplate.from_messages(
    [
        ("system", _answer_combiner_system_prompt2),
        (
            "human",
            """##INPUT:\nOriginal Question: {original_question}

Question Answer Pairs:\n{decomposed_qa_pairs}\n

##OUTPUT: """,
        ),
    ]
)
from langchain_core.messages import SystemMessage, HumanMessage
answers_combiner2 = answers_combination_prompt2 | llm.with_structured_output(
    CombinedAnswer
)

def combine_answer_v3(state: state.OverallState):
    log_message(
        "---COMBINING ALL THE DECOMPOSED ANSWERS TO ANSWER ORIGINAL QUESTION---"
    )

    """
    if(state['final_answer']):
        log_message(f"Combined answer: {state['final_answer']}")
        delete_messages(state)
        reset_state_except_final_and_messages(state)
        return {"final_answer": state['final_answer'], "messages": [AIMessage(role="Chatbot",content=combined_answer)]
                ,"clarifying_questions":[]}
    """

    # question_trees=state['question_tree_store']
    qa_pairs = "\n".join(state["qa_pairs"])

    original_question = state["question"]
    image_url=state.get("image_url","")
    if state.get("image_url","")!="":
        image_url = f"data:image/jpeg;base64,{image_url}"
        answers_combination_prompt2_with_image = ChatPromptTemplate.from_messages(
            messages = [
                SystemMessage(content=_answer_combiner_system_prompt2),
                HumanMessage(content=[
                    {
                        "type": "text",
                        "text": f"Question Answer Pairs:\n{qa_pairs}\n",
                    },
                    {
                        "type": "image_url",
                        "image_url": {f"url: {image_url}"}
                    },
                    {
                        "type": "text",
                        "text": f"Question: {original_question}"
                    }
                ])
            ]
        )
        answers_combiner2_with_image = answers_combination_prompt2_with_image | llm.with_structured_output(
            CombinedAnswer
        )
        combined_answer=answers_combiner2_with_image.invoke({})
    else:
        combined_answer = answers_combiner2.invoke(
            {"original_question": original_question, "decomposed_qa_pairs": qa_pairs}
        )

    combined_answer = combined_answer.combined_answer

    ###### log_tree part

    id = str(uuid.uuid4())
    child_node = nodes.combine_answer_v3.__name__ + "//" + id
    parent_node = state.get("prev_node", "START")
    log_tree = {}
    log_tree[parent_node] = [child_node]
    ######

    ##### Server Logging part

    if not LOGGING_SETTINGS["decomposer_node_3"]:
        child_node = parent_node

    output_state = {
        "final_answer": combined_answer,
        "messages": [AIMessage(role="Chatbot", content=combined_answer)],
        "clarifying_questions": [],
        "prev_node": child_node,
        "log_tree": log_tree,
    }

    send_logs(
        parent_node=parent_node,
        curr_node=child_node,
        child_node=None,
        input_state=state,
        output_state=output_state,
        text=child_node.split("//")[0],
    )

    ######

    log_message(f"Combined answer: {combined_answer}")
    delete_messages(state)
    reset_state_except_final_and_messages(state)

    return output_state


### SEMANTIC CACHING [REPEATER_WITH_CACHE]

cache_answer_system = prompts.cache_answer_system

cache_answer_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", cache_answer_system),
        (
            "human",
            """##INPUT
Question:{question}
Answers:
{answers}

##OUTPUT: """,
        ),
    ]
)

cache_answer = cache_answer_prompt | llm.with_structured_output(CacheSufficient)

def cache_retriever_call(query):
    try:
        docs=cache_retriever.similarity_search(
            query,
            config.cache_retriever_DOCS,
            metadata_filter=nodes.convert_metadata_to_jmespath({"is_cache":"True"})
            )
    except:
        return 'no'
    #print(docs)
    answers_str=[f"{no}. {i.metadata['answer']}" for no,i in enumerate(docs)]
    #print(query)
    #print(answers_str)
    index=cache_answer.invoke(
        {
            "question":query,
            "answers":'\n'.join(answers_str)
        }
    ).index

    if index == -1:
        return "no"

    else:
        entry=docs[index]
        answer=entry.metadata['answer']
        final=answer
        #final={'answer':answer}
    
    #print('FINAL',final)

    return final
    
#making the node for it

def cache_retriever_node(state:state.InternalRAGState ):

    output=cache_retriever_call(state['question'])
    answer=''
    if output!='No':
        answer=output

    return{
        'cache_output':output,
        'final_answer':answer,
        'answer':answer
    }
