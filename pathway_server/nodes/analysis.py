# This script defines several functions and models for handling and generating questions 
# related to analysis based on Key Performance Indicators (KPIs) and prompts. 

# Key components:
# 1. ARQuestions - A Pydantic model that represents the decomposed questions required for analysis.
# 2. extract_KPIs_from_fields - Extracts and aggregates KPIs from given fields based on exact matches.
# 3. generate_KPI_based_questions - A placeholder function for generating questions based on KPIs (currently not implemented).
# 4. get_relevant_questions - Extracts relevant questions for analysis and sends them to the RAG pipeline.
# 5. get_relevant_questions_v2 - An updated version of get_relevant_questions that uses domain knowledge for extracting performance metrics.
# 6. FinalAnswer - A Pydantic model that holds the final answer derived from the analyses.
# 7. combine_analysis_questions - Combines RAG responses and generates a final answer using LLM.
# 8. append_analysis_questions - Collects and joins RAG responses for later use.

from typing import List, Tuple
from pydantic import BaseModel, Field
from langchain_core.prompts import ChatPromptTemplate
import state
from llm import llm
from nodes.KPIs import possible_KPIs
from functools import reduce
from prompt import prompts

class ARQuestions(BaseModel):
    """The structure for decomposed question generated by the decomposing the original question."""

    ar_questions: List[str] = Field(
        description="Generated questions about Analysis required by Analysts to solve the original question."
    )


## assumes exact match
def extract_KPIs_from_fields(fields: List[str]) -> Tuple[List[List[str]], List[str]]:
    kpi_list = list(map(lambda k: possible_KPIs.get(k, {}).get("KPIs", [])))
    aggregated = list(reduce(lambda x, y: x | y, kpi_list))
    return kpi_list, aggregated


_ar_questions_prompt_kpi = prompts._ar_questions_prompt_kpi

## TODO ##
def generate_KPI_based_questions(state: state.OverallState):
    """
    Generates questions based on the KPIs for the user to answer. How much should this be LLM dependent?

    Currently the LLM decides how to generate the questions based on the KPIs.
    """
    pass
    ### check if you need to


_ar_questions_prompt = prompts._ar_questions_prompt


def get_relevant_questions(state: state.OverallState) -> state.InternalRAGState:
    """
    Extracts all relevant questions for analysis
    Sends them to the RAG pipeline
    """
    prompt = _ar_questions_prompt.format(
        question=state["question"],
        analysis_types=state["user_response_for_analysis"],
        analyst_info=", ".join(analyst.role for analyst in state["analysts"]),
    )
    generator = llm.with_structured_output(ARQuestions)

    answer = generator.invoke(prompt)
    query_list = answer.ar_questions
    # state["analysis_question_groups"] = query_list
    # return {"decomposed_questions":query_list}
    return {"analysis_question_groups": query_list}


_ar_v2_questions_prompt = prompts._ar_v2_questions_prompt


def get_relevant_questions_v2(state: state.OverallState) -> state.InternalRAGState:
    """Uses domain knowledge for extracting performance metrics"""
    prompt = _ar_v2_questions_prompt.format(
        question=state["question"],
        analysis_types=state["user_response_for_analysis"],
        analyst_info=", ".join(analyst.role for analyst in state["analysts"]),
    )
    generator = llm.with_structured_output(ARQuestions)

    answer = generator.invoke(prompt)
    query_list = answer.ar_questions
    # state["analysis_question_groups"] = query_list
    # return {"decomposed_questions":query_list}
    return {"analysis_question_groups": query_list}


class FinalAnswer(BaseModel):
    answer: str = Field(
        description="Final answer based on individual analyses of the personas."
    )


_conclusion_prompt = prompts._conclusion_prompt


## This may be redundant wrt the combiner in persona.py
def combine_analysis_questions(state: state.InternalRAGState) -> state.OverallState:
    """
    Joins RAG responses and asks the LLM to Generate the response
    """
    context_questions = state["analysis_subquestions"]
    context_responses = state["analysis_subresponses"]
    qa_pairs = "####\n".join(
        [f"{quer}:\n{ans}\n" for quer, ans in zip(context_questions, context_responses)]
    )
    prompt = _conclusion_prompt.format(question=state["question"])
    structured_prompt = ChatPromptTemplate.from_messages(
        [("system", prompt), ("human", "Analyses: {context}")]
    )
    answer_generator = structured_prompt | llm.with_structured_output(FinalAnswer)
    answer = answer_generator.invoke({"context": qa_pairs})
    return {"final_answer": answer.answer}


## OLD CODE ###
def append_analysis_questions(state: state.InternalRAGState) -> state.OverallState:
    """
    Collects and joins RAG responses
    """
    context_questions = state["analysis_subquestions"]
    context_responses = state["analysis_subresponses"]

    qa_pairs = "####\n".join(
        [f"{quer}\n{ans}" for quer, ans in zip(context_questions, context_responses)]
    )
    return {"analysis_retrieved_context": qa_pairs}
