"""
This module defines a Python code execution framework and a calculator agent to evaluate tasks and process answers with suggestions.

Core Components:
1. **Code Generation and Execution:**
   - `execute_task_and_get_result`: Accepts a task description, generates Python code using GPT-4's chat-based API, executes the code in a restricted environment, and returns the result. The function includes retry logic in case of errors during code generation or execution.
   - `code_generator`: A LangChain prompt template used for generating Python code via GPT-4's chat completion endpoint.

2. **Calculator Agent:**
   - **Purpose:** The `calc_agent` function evaluates a question and its associated answer using a series of operations provided by a calculator model. It either directly processes the operations or suggests further processing steps.
   - **Components:**
     - `calculator_input`: A LangChain prompt template that takes in a question and answer, invoking a calculator model to generate suggested operations.
     - `process_answer`: A function that processes the suggestions generated by the calculator and incorporates them into a final answer.

3. **Models for Structured Output:**
   - `CalculatorOutput`: A Pydantic model that stores the list of operations or instructions returned by the calculator.
   - `ProcessAnswer`: A Pydantic model that holds the final answer after applying suggestions from the calculator.

4. **Logging and Error Handling:**
   - The module includes logging functionality to track task execution, code generation, and error messages using `log_message`.
   - It also features retry logic for both code generation and execution (up to three attempts).

5. **Prompts:**
   - Utilizes LangChain's `ChatPromptTemplate` to interact with the GPT-4 model. Specific prompts are designed for generating Python code, processing calculator suggestions, and evaluating the final answer.

Dependencies:
- **LangChain** for managing prompts and invoking GPT-4's chat API.
- **Pydantic** for structured data models.
- **Internal utilities** (`log_message`, `llm`, `state`) for logging and managing execution state.

Usage:
- Call `execute_task_and_get_result(task)` to generate Python code for a specific task and return the result.
- Use `calc_agent(state)` to process answers and generate final suggestions based on calculator model outputs.
"""

from langchain_core.prompts import ChatPromptTemplate
from utils import log_message
import state
from llm import llm
from langchain_core.tools import tool
from state import Value, Give_Output
from .calculator import execute_task_and_get_result
from prompt import prompts
# ----------------- prompts -----------------#

is_visualizable_prompt = prompts.is_visualizable_prompt

get_metrics_prompt = prompts.get_metrics_prompt

get_metrics_value_prompt = prompts.get_metrics_prompt

get_insights_prompt = prompts.get_insights_prompt

get_charts_name_prompt = prompts.get_charts_desc_prompt

get_bar_chart_prompt = prompts.get_bar_chart_prompt

get_line_chart_prompt = prompts.get_line_chart_prompt

get_pie_chart_prompt = prompts.get_pie_chart_prompt


# ----------------- llm -----------------#

llm_bool = llm.with_structured_output(state.Visualizable)
llm_metrics_structured = llm.with_structured_output(state.Metrics)
llm_insights_structured = llm.with_structured_output(state.Insights)
llm_gen_charts_structured = llm.with_structured_output(state.GenCharts_instructions)
llm_code_structured = llm.with_structured_output(state.CodeFormat)
llm_gen_charts_name_structured = llm.with_structured_output(state.Chart_Name_data)
get_bar_chart_structured = llm.with_structured_output(state.BarChart)
get_line_chart_structured = llm.with_structured_output(state.LineChart)
get_pie_chart_structured = llm.with_structured_output(state.PieChart)

# ----------------- nodes -----------------#


def is_visualizable_route(state: state.VisualizerState):
    log_message("--- IS VISUALIZABLE ROUTE ---")
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", is_visualizable_prompt),
            ("user", "Here is the input data: {input}"),
        ]
    )
    is_visualizable_route = prompt | llm_bool
    # 2sec to exec
    response = is_visualizable_route.invoke({"input": state["input_data"]})
    return {"is_visualizable": response}


def get_metrics(state: state.VisualizerState):
    log_message("--- GET METRICS ---")
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", get_metrics_prompt),
            ("user", "Here is the input data: {input}"),
        ]
    )
    metrics_descriptor = prompt | llm_metrics_structured
    # 8sec to exec
    response = metrics_descriptor.invoke({"input": state["input_data"]})
    return {"metrics": response.output}


def get_metric_value(state: state.Metric_Value):
    log_message("--- GET METRIC VALUE ---")
    task = f"""
        Given the following details:

        - **Metric Name**: {state["metric"].metric_name}
        - **Metric Description**: {state["metric"].metric_description}
        - **Data Required**: {state["metric"].data_required}

        Calculate the value of the metric using the provided data and return the result.
        """
    # 3sec * 6
    answer = execute_task_and_get_result(task)["answer"]
    return_ans = Value()
    if isinstance(answer, float) or isinstance(answer, int):
        return_ans.name_of_the_metric = state["metric"].metric_name
        return_ans.value = answer
        ans = get_insights(state["metric"].metric_name, answer, state["input_data"])
        return {"values": [return_ans], "final_insights": ans["final_insights"]}
    return {"values": [return_ans], "final_insights": [""]}


def get_insights(metric_name, metric_value, input_data):
    log_message("--- GET INSIGHTS ---")
    prompt_for_insight = f"""
        Given the following details:
        - **Metric Name**: {metric_name}
        - **Metric Value**: {metric_value}
        - **All the data**: {input_data}
        """
    prompt = ChatPromptTemplate.from_messages(
        [("system", get_insights_prompt), ("user", prompt_for_insight)]
    )
    insights_generator = prompt | llm_insights_structured
    if metric_name == "":
        {"final_insights": [""]}
    response = insights_generator.invoke(
        {
            "input_data": input_data,
            "metric_name": metric_name,
            "metric_value": metric_value,
        }
    )
    if response.grade <= 2:
        return {"final_insights": [""]}
    return {"final_insights": [response.insights]}


def get_final_insights(state: state.VisualizerState):
    final_answer = state["final_insights"]
    prompt_for_final_answer = (
        "The following insights have been noted, listed in no particular order:\n"
    )
    prompt_for_final_answer += "\n".join(
        [f"{i+1}. {final_answer[i]}" for i in range(len(final_answer))]
    )
    prompt_for_summary = """
    Extract the key points from the following insights, focusing on results that are important for financial analysis. Present them in a concise and easy-to-read bullet point format. If multiple insights convey similar information, or if they are sequential or imply each other, combine them into a single point. Avoid repeating similar information or stating redundant points. Ensure each point is based only on the provided data, with no additional generation or assumptions. Make the key points clear, distinct, and straightforward to help quickly grasp the essential insights, emphasizing the financial outcomes and implications.
  """
    prompt = ChatPromptTemplate.from_messages(
        [("system", prompt_for_summary), ("user", prompt_for_final_answer)]
    )
    # 5sec
    final_answer_generator = prompt | llm.with_structured_output(Give_Output)
    result = final_answer_generator.invoke({})
    return {"final_output": result.output}


def get_charts_name(state: state.VisualizerState):
    log_message("--- GENERATING CHART NAMES ---")
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", get_charts_name_prompt),
            ("user", "Here is the input data: {input_data}"),
        ]
    )
    chart_name_generator = prompt | llm_gen_charts_name_structured
    # 2sec
    response = chart_name_generator.invoke({"input_data": state["input_data"]})
    return {"chart_names": response.data}


def get_charts_data(state: state.Chart_Name_for_data):
    log_message("--- GET CHARTS DATA ---")
    prompt = ""
    structured = ""
    if state["state"].type == "Bar Chart":
        prompt = ChatPromptTemplate.from_messages(
            [
                ("system", get_bar_chart_prompt),
                ("user", "Here is the input data: {input_data}"),
            ]
        )
        structured = get_bar_chart_structured

    elif state["state"].type == "Line Chart":
        prompt = ChatPromptTemplate.from_messages(
            [
                ("system", get_line_chart_prompt),
                ("user", "Here is the input data: {input_data}"),
            ]
        )
        structured = get_line_chart_structured
    else:
        prompt = ChatPromptTemplate.from_messages(
            [
                ("system", get_pie_chart_prompt),
                ("user", "Here is the input data: {input_data}"),
            ]
        )
        structured = get_pie_chart_structured

    generator = prompt | structured
    try:
        # 2.6sec * 2
        response = generator.invoke(
            {"input_data": state["input_data"], "title": state["state"].title}
        )
    except:
        response = ""
    return {"charts": [response]}
